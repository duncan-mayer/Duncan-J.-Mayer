[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Duncan J. Mayer",
    "section": "",
    "text": "Easy Inference with Informal Bayes\n\n\n\n\n\n\n\nbayesian inference\n\n\nsimulation\n\n\nbase\n\n\nr\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2022\n\n\nDuncan J. Mayer\n\n\n\n\n\n\n  \n\n\n\n\nPublications\n\n\n\n\n\n\n\npublications\n\n\npdf\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nDuncan J. Mayer\n\n\n\n\n\n\n  \n\n\n\n\nStarting a Quarto Blog\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2022\n\n\nDuncan J. Mayer\n\n\n\n\n\n\n  \n\n\n\n\nOn the Adjacency Matrix in ICAR and Convolution Prior Models\n\n\n\n\n\n\n\nspatial\n\n\nsimulation\n\n\nr\n\n\nbase\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2022\n\n\nDuncan J. Mayer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/publications/index.html",
    "href": "posts/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Here is a list of my publications and select presentations, with links to PDFs. I will updated it periodically."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Starting a Quarto Blog",
    "section": "",
    "text": "This blog is my transition to quarto, and a place I’ll write about topics that interest me, or things I’m learning. Check out the my about page to learn more about me.\nSome items and topics that may appear on this blog include descriptions of presentations I make, or publications I contribute to. I may also write about spatial statistics, nonlinearity, and select topics in statistical inference.\nAdditionally, I’d like to use this blog to practice programming in a more public space. I have nearly a decade of experience with R, but I’d like to improve with python, and explore popular workflows in R that I haven’t yet. So, some posts related to those topics may appear here as well.\n\n\n\nGlasgow Scotland, 1955"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Duncan J. Mayer, a data scientist with nearly a decade of experience using data to predict and understand complex behavior and phenomenon. I maintain good coding practices and am comfortable building, evaluating, and interpreting everything from well known algorithms to bespoke models. In addition to my methodological work, I bring domain expertise in organizational theory and have published articles on nonprofit finance, data use, and neighborhood effects.\nI’m a statistician at BeVera Solutions and a PhD candidate with the Center on Poverty and Community Development in the Jack Joesph, and Morton Mandel School of Applied Social Sciences at Case Western Reserve University"
  },
  {
    "objectID": "posts/publications/index.html#publications",
    "href": "posts/publications/index.html#publications",
    "title": "Publications",
    "section": "Publications",
    "text": "Publications\nMayer, D. J. (2023) Social Capital and the Nonprofit Infrastructure; an Ecological Study of Child Maltreatment. Journal of Community Psychology. https://doi.org/10.1002/jcop.22984 pdf\nMayer, D. J., Groza, V. (in press) Promoting Children’s Rights in Program Evaluation. American Journal of Evaluation.\nMayer, D. J., & Fischer, R. L. (2023). Exploring data use in nonprofit organizations. Evaluation and Program Planning, 97, 10. https://doi.org/10.1016/j.evalprogplan.2022.102197 pdf\nMayer, D. J. (2022) Simmer Down Now! A Study of Revenue Volatility and Dissolution in Nonprofit Organizations. Nonprofit and Voluntary Sector Quarterly. https://doi.org/10.1177/08997640221126147 pdf\nMayer, D. J., Fischer, R. L. (2022) Can a Measurement Error Perspective Improve Estimation in Neighborhood Effects Research? A Hierarchical Bayesian Methodology. Social Science Quarterly. https://doi.org/10.1111/ssqu.13190 pdf"
  },
  {
    "objectID": "posts/publications/index.html#select-presentations",
    "href": "posts/publications/index.html#select-presentations",
    "title": "Publications",
    "section": "Select Presentations",
    "text": "Select Presentations\nMayer, D. J. (2022). Understanding the Geography of Cuyahoga County’s Nonprofit Sector Through Form-990 Data. Data Days Cleveland. slides"
  },
  {
    "objectID": "posts/Easy Inference with Informal Bayes/index.html",
    "href": "posts/Easy Inference with Informal Bayes/index.html",
    "title": "Easy Inference with Informal Bayes",
    "section": "",
    "text": "A Simple Application\nHaving built a function to sample the posterior of regression parameters, lets see how this would play out in practice. Consider the following simplified situation where we have two important variables representing factors that exist simultaneously. For example, these may be the amount of resources allocated to two departments, or two types of amenities you offer patrons. The simultaneously deployment is important for the ceteris paribus interpretation, as well as for the setup, otherwise the model could be constructed as a choice between two options. So, say we wish to know which amenity to build out further, or which department to would benefit the company more if given additional resources. It’s a slightly unconventional question, and staring at the estimates of a linear model won’t give you any insight as the results aren’t calibrated for such a comparison. Rather, to prepare for such an analysis, we might take the approach of an informal power analysis to estimate the required sample (e.g., number of locations, employees, or whatever the unit happens to be).\nThe following function takes a vector (bx, the fixed, but unknown conditional mean parameters), a variance-covariance matrix (sigma), a constant (const), as well as number of samples (n), and produces a linear model from the results. In the example I’ve kept it to three covariates including our two focal variables, without loss of generality.\n\n\nCode\nsimple_sim <- function(n = 1e3, bx = c(2,6,4), const = 5,\n                       mu = c(10, 3, 4), \n                       sigma = matrix(\n                               data = c(1.11, 0.66, -0.54,  \n                                        0.66,  0.84,  0.43, \n                                        -0.54,  0.43,  2.00), nrow = 3)\n                       ) {\n  # check mu and sigma\n  if (length(mu) != dim(sigma)[1]) {stop(\"mu and sigma are noncomforable\")}\n  if (!is.matrix(sigma) | (nrow(sigma) == ncol(sigma)) == FALSE) {stop(\"sigma either not a matrix or not a square matrix\")}\n  # create data\n  dft <- MASS::mvrnorm(n, mu = mu, Sigma = sigma)\n  yy <- as.vector(rnorm(n, const, const) + dft %*% bx + rnorm(n))\n  dft <- as.data.frame(dft)\n  names(dft) <- letters[1:length(names(dft))]\n  names(dft)[1:2] <- c(\"x1\", \"z1\")\n  dft <- cbind(yy, dft)\n  # fit model\n  lmt <- lm(yy ~ ., data = dft)\n  return(lmt)\n}\n\n\nNext, I generate some candidate sample sizes, ranging from 100 to 1000, incremented by 100. Typically, smaller samples are less resource intensive, so I’d like as small a sample as possible without sacrificing the precision of the estimated difference. Then, I apply the function to sample the posterior, the first five rows of which are shown below, where the samples column represents the sample size used in them model.\n\n\nCode\nset.seed(89)\nn_rep <- seq(0,1e3, by = 100)[-1]\nout <- list()\n for (i in seq_along(n_rep) ) {\n    out[[i]] <- simple_sim(n = n_rep[[i]])\n }\ndfout <- lapply(out, infbayes)\nfor (i in seq_along(dfout)) {\n  dfout[[i]]$samples <- n_rep[[i]]\n}\ndfout <- do.call(rbind.data.frame, dfout)\nhead(dfout,5)\n\n\n  (Intercept)       x1       z1        c samples\n1   -37.27578 6.713368 1.169126 6.607417     100\n2   -22.99647 5.028081 3.205434 5.264976     100\n3   -37.89262 6.541961 1.999250 6.241230     100\n4   -14.29463 3.982110 4.065508 5.451617     100\n5   -29.11941 6.038991 1.042586 5.972961     100\n\n\nHaving conducted informal Bayes, the results can be easily plotted, as in the figure below. The figure gives us a feel for the implications of the sample size for the difference in conditional mean parameters in a linear model. The results show that, under the assumptions of the model and a difference of 4 with no heterogeneity, you’d probably want a sample of at least 700. If those assumptions aren’t reasonable, or you need additional covariates, then you can adjust the simulation.\n\n\nCode\nlibrary(ggplot2)\ndfout$tdiff <- dfout$z1 - dfout$x1\n\nggplot(data = dfout, aes(tdiff)) + geom_histogram(binwidth = .01, fill = \"black\") +\n  facet_wrap(~ samples, nrow = 2) + labs(x = \"Difference\", y = \"Frequency\") +\n            theme_bw() +\n              theme(legend.position = \"none\", text=element_text(family=\"Times New Roman\", face=\"bold\", size=12))\n\n\n\n\n\n\n\nReferences\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press; Cambridge Core. https://doi.org/10.1017/CBO9780511790942\nKing, G., Tomz, M., & Wittenberg, J. (2000). Making the Most of Statistical Analyses: Improving Interpretation and Presentation. American Journal of Political Science, 44(2), 15.\n\n\n\n\n\nFootnotes\n\n\nWhy multivariate normal? A general justification often relies on the central limit theorem under finite variance, in this example using OLS, the sampling distribution of the conditional mean parameters are known to be marginally normal and jointly multivariate normal with variance-covariance equal to \\(\\sigma^2 (X'X)^{-1}\\)↩︎"
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "",
    "text": "This post explores the role of the adjacency matrix in determining spatial variation in the Intrinsic Conditional Auto-regressive (ICAR) and convolution prior models."
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#the-adjacency-matrix",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#the-adjacency-matrix",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "The Adjacency Matrix",
    "text": "The Adjacency Matrix\nThe precision matrix for the multivariate normal that defines the joint distribution of the spatial effects is derived from a diagonal matrix and an adjacency matrix, both are \\(n\\) by \\(n\\) matrices, where the diagonal matrix contains the number of neighbors corresponding to neighborhood \\(i\\) in entry \\(n_{ii}\\) while the adjacency matrix is often binary, containing a “1” in entry \\(n_{ij}\\) if neighborhoods \\(i\\) and \\(j\\) are neighbors, and “0” otherwise2.\nClearly, the specification of the adjacency matrix has implications for the estimates of \\(\\phi_i\\). In the equation for the ICAR shown above, it is clear that the total number of neighbors can change the estimates: the specification of neighbors in the sum change the estimates for the mean, as it is calculated conditional on the neighbors. It is likely that a high number of neighbors increases the spatial effects to the extent that they outpace the increase in the denominator. Further, the variance decreases as a function of the number of neighbors defined in the adjacency matrix."
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#the-convolution-prior-model",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#the-convolution-prior-model",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "The Convolution Prior Model",
    "text": "The Convolution Prior Model\nThe ICAR prior assumes complete spatial autocorrelation, which can be viewed as a is a strong prior on the source of variation when included in isolation: fixing non-spatial variation at zero. The convolution prior (Besag, York, and Mollié, 1991) relaxes this assumption by including a spatially structured effect (ICAR) and an unstructured random effect that varies over the spatial units. Commonly, this comes in the form of the Besag-York-Mollié (BYM) model, which places the two random effects in the context of a log-Poisson model, often with an exposure term. With covariates outcome \\(Y_i\\) for spatial units \\(i\\), covariates \\(X\\) and offset \\(p_i\\), the BYM can be written as\n\\[\nY_i \\sim Poisson(\\lambda_i)\n\\]\n\\[\nln(\\lambda_i) \\equiv ln(p_i) + X\\beta + \\phi_i + \\delta_i\n\\]\n\\[\n\\phi_i | \\phi_{[i]} \\sim N \\left(\\frac{\\sum_{[i]} \\phi_i }{d_i},\n\\frac{\\sigma_\\phi^2}{d_i} \\right) \\; \\textrm{and} \\;\n\\delta_i \\sim N(0, \\sigma^2_{\\delta})\n\\]\nThe convolution prior model often requires sensible priors, estimating two random effects when one measurement is taken from each spatial unit the posteriors may be sensitive to the selection of priors in the full Bayesian context. Additionally, sensible priors (or “fair”, in the sense that they equally weight spatial and non-spatial variation) are difficult to generate as the ICAR is specified conditionally while the unstructured effect is specified marginally3. However, it is often tempting to compare the sources of variation, declaring a percent of the variation in the outcome due to spatially structured variation, or non-spatially structured variation. For example, one may compute \\(\\sigma^2_\\phi / (\\sigma^2_\\phi + \\sigma^2_\\delta)\\). Yet, as described above, the posterior of the variance may depend heavily on the specification of the adjacency matrix, an issue under-researched in the literature. Accordingly, this post explores the sensitivity of the posterior estimates to the specification of the adjacency matrix, as well as the ability of the model to recover the correct neighborhood structure."
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#defining-adjacency-matrices",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#defining-adjacency-matrices",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "Defining Adjacency Matrices",
    "text": "Defining Adjacency Matrices\nTo simulate the convolution prior, I first must specify a neighborhood graph. For the purpose of this post, I’m going to use Cuyahoga County Ohio’s Census Tracts, with 2010 geographic boundaries. I will generate a total of 6 adjacency matrices to show how rapidly the number of neighbors increase, although in the eventual in the simulation study, I will focus on “queen” contiguity as it is the most commonly used, and entertaining all patterns would be much too long for a post, although the idea holds for other patterns.\nI define all adjacency matrices to be binary, varying the order of neighbors from 1-3. Below, I’ve written a function to help create the various adjacency matrices that will be used in this study.\n\n\nCode\nlibrary('ggplot2')\nlibrary('INLA')\n\nset.seed(101) # reproducibility\nca <- tigris::tracts(state = \"OH\", county = \"Cuyahoga\", year = 2010)\n# remove water \nca <- ca[ca$TRACTCE10 !=  990000,]\nca$areaid <- ca$TRACTCE10\n\n# helper function for weight matrix \ncreate_neighborhood_matrix <- function(geo, geo_names, queen = TRUE, style = \"B\", order = 1) {\n  if (class(geo)[1] != \"sfc_MULTIPOLYGON\") {stop(\"geo must be of class sfc_MULTIPOLYGON\")} \n  w <- spdep::poly2nb(geo, row.names = geo_names, queen = queen)\n  if (class(w) != \"nb\") {stop(\"w must be of class nb\")}\n  if (order < 1 | order > 3) {stop(\"order of neighbors must be 1, 2, or 3\")}\n  if (order == 1) {mat <- spdep::nb2mat(w, style = style)}\n  if (order %in% c(2, 3)) {\n      wl <- spdep::nblag(w, maxlag = order)\n      wl <- spdep::nblag_cumul(wl)\n      mat <- spdep::nb2mat(wl, style = style)\n  }\n  return(mat)\n}\n\n\nAs I define the neighbors to be of higher order, the average number of neighbors increases dramatically, for example, the average number of neighbors for each unit increases from 6 to 41 using queen contiguity, and from 5 to 36 for rook contiguity, as we vary the order of neighbors from 1 to 3.\n\n\nCode\noutq <- list()\noutr <- list()\nfor ( i in 1:3) {\noutq[[i]] <- create_neighborhood_matrix(geo = ca$geometry, geo_names = ca$TRACTCE10, order = i)\n\noutr[[i]] <- create_neighborhood_matrix(geo = ca$geometry,queen = FALSE, geo_names = ca$TRACTCE10, order = i)\n}\nnlist <- c(outq, outr)\ndfoutq <- do.call(rbind.data.frame, lapply(outq, \\(x) sum(x) / dim(x)[1]))\ndfoutr <- do.call(rbind.data.frame, lapply(outr, \\(x) sum(x) / dim(x)[1]))\noutn <- cbind(dfoutq, dfoutr)\nnames(outn) <- c(\"Queen\", \"Rook\")\noutn$Order <- c(1:3)\ngt::gt(round(outn, 2))\n\n\n\n\n\n\n  \n  \n    \n      Queen\n      Rook\n      Order\n    \n  \n  \n    6.10\n5.34\n1\n    19.63\n17.17\n2\n    41.58\n36.34\n3"
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#simulating-the-convolution-prior",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#simulating-the-convolution-prior",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "Simulating the Convolution Prior",
    "text": "Simulating the Convolution Prior\nIn the code below, for a given adjacency matrix, I create a function with simulates data consistent with the neighbor graph following the process of a convolution prior on the log scale, with 3 covariates sampled from a multivariate normal and an offset sampled from a uniform distribution. In the code below, the spatial effect is generated using SUMMER and labeled sp while the unstructured variation is unstruc. There are two features worth noting here. First, the random effects are uncorrelated with the covariates, a simplifying assumption of the BYM model. Second, the unstructured variation is sampling from a normal distribution with standard deviation defined as equal to the standard deviation of the spatially structured variation. This ensures they are equal, on average, when the data are simulated.\n\n\nCode\nsimulate_bym_df <-  function(neighborhoodmat, n = dim(neighborhoodmat)[1], bx = c(.08,-.03,-.04),\n                                   mu = c(1, 1, 2), \n                                   sigma = matrix(\n                                     data = c(1.11, 0.30, -0.15,  \n                                              0.30,  0.84,  0.25, \n                                              -0.15,  0.25,  2.00), nrow = 3)) {\n  # create data\n  dft <- MASS::mvrnorm(n, mu = mu, Sigma = sigma)\n  sp <- as.vector(SUMMER::rst(n= 1, n.t = 1, type = \"s\", \n                              Amat = neighborhoodmat, scale.model = TRUE)) # spatial random effect, ICAR\n  offset_pop <- round(runif(n, min = 100, max = 1000), 0) # population size\n  unstruc <- rnorm(n, 0, sd(sp)) # simulate normal, unstructured random effect with standard deviation equal to spatial variation\n  mu_p <- exp(0 + 1*log(offset_pop) + dft %*% bx + sp + unstruc) # generate mean function \n  yy <- as.vector(rpois(n, lambda = mu_p)) # poisson \n  dft <- as.data.frame(cbind(dft, yy, sp, unstruc, offset_pop, mu_p)) # package data\n  dft$areaid <- row.names(neighborhoodmat) # create ids\n  return(dft)\n}\n\n\nWith functions to create a neighborhood structure of our choice and generate data following the convolution prior with ecological covariates, all that’s left to do before fitting the models is create the data.\n\n\nCode\n# create data \ndfqlist <- list()\nsdq <- list()\nfor ( i in 1:3) {\n# underlying structure of spatial correlation is queen with order 1-3\ndfqlist[[i]] <-  simulate_bym_df(neighborhoodmat =  \n                                create_neighborhood_matrix(geo = ca$geometry, geo_names = ca$TRACTCE10, order = i))\n\n}\ndflist <- dfqlist\n#"
  },
  {
    "objectID": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#results",
    "href": "posts/On the Adjacency Matrix in ICAR and Convolution Prior Models/icar_note.html#results",
    "title": "On the Adjacency Matrix in ICAR and Convolution Prior Models",
    "section": "Results",
    "text": "Results\nIn the code below I fit models model with the convolution prior using 3 data sets, each 3 times, varying only the specification of the adjacency matrix4. I use Integrated Nested Laplace Approximation (INLA) with the vague priors that inla has as default (diffuse log-gamma for spatial and non-spatial variance). INLA uses optimization rather than sampling, and while I typically prefer the full Bayesian methodology, INLA is often very efficient for latent Gaussian models and I’m fitting several of them here.\nHaving fit the 9 models, I then extract the hyper-parameters transformed (which are transformed into standard deviation) and show the results in the figure below. The top row of the figure shows a summary of the posterior for the standard deviation for the standard deviation of the non-spatial effect, while the second row shows the standard deviation of the spatial variation. The number underneath the label for the source of variation indicates the order of neighbors used to simulate the ICAR.\nThere are several notable features of the posterior summaries. As I suggested when inspecting the model in the first section, in all cases the posterior interval of the spatial variation decreases as the order of neighbors, and the average number of neighbors, increases. Additionally, at least when the true order of neighbors is above 1, the standard deviation of the spatial effect increases linearly with the order of neighbors assumed in the construction of the adjacency matrix. When the true order is 1, however, the standard deviation increases when the order jumps for 1 to 2, but not from 2 to 3. As the two random effects in the BYM model compete for variation, the non-spatial standard deviation decreases as a function of the number of neighbors. In this sense, the simulation shows that the percent of variance that is spatially structured is largely an artifact of the adjacency matrix.\n\n\nCode\nfitf <- function(neigh, df) {\n  #  INLA FORMULA\nform <- yy ~ 1 + V1 + V2 + V3 + \n  f(areaid, model = \"bym\", \n  graph = neigh, values = row.names(neigh), scale.model = TRUE) \nm <- inla(formula = form,\n     family= \"poisson\", offset = log(offset_pop),\n     data=df,\n     control.family=list(link='log'),\n     control.predictor=list(link=1, compute=TRUE),\n     control.compute=list(dic=TRUE, cpo=TRUE, waic=TRUE))\nmout <- as.data.frame(brinla::bri.hyperpar.summary(m))\nmout$empircal_sdsp <- sd(df$sp)\nmout$empircal_sdun <- sd(df$unstruc)\nmout$waic <- m$waic$waic\nreturn(mout)\n}\n# index book keeping to avoid nested loop\ntr <- expand.grid(1:3, 1:3)\nmodlist <- list()\nfor (i in 1:length(tr$Var1) ) {\nmodlist[[i]] <- fitf(nlist[[tr$Var1[[i]]]], dflist[[tr$Var2[[i]]]])\n}\n\nnames(tr) <- c(\"neigh\", \"df\")\nhyper_results <- do.call(rbind.data.frame, modlist)\n\n# label componenets\nhyper_results$com <- rep(c(\"iid Variation\",\"Spatial Variation\"))\n\n# combine results \nhyper_results <- rbind(cbind(hyper_results[hyper_results$com == \"iid Variation\",], tr), cbind(hyper_results[hyper_results$com == \"Spatial Variation\",], tr))\nhyper_results$neigh <- as.factor(hyper_results$neigh )\n\n# plot hyper-parameters over adjacnecy specifications\nggplot(hyper_results, aes(y = mean, x = neigh, \n                          ymin = q0.025, ymax = q0.975)) + geom_pointrange() + \n  labs(y = \"Mean and Posterior Interval\", x = \"Order of Neighors (assumed)\") +\n  facet_wrap(. ~ com + df, scales = \"fixed\") + theme_classic()"
  }
]